---
layout: default
title: Invited Speakers
---

<style>
.speaker-row {
  margin-bottom: 0.2rem;
  padding-bottom: 0.2rem;
  padding: 0.5rem;
  border-radius: 4px;
  display: flex;
  align-items: flex-start;
}

.speaker-row:nth-child(even) {
  background: linear-gradient(135deg, transparent 0%, rgba(144, 86, 86, 0.08) 50%, rgba(144, 86, 86, 0.16) 100%);
}

.speaker-row:nth-child(odd) {
  background: linear-gradient(135deg, rgba(144, 86, 86, 0.16) 0%, rgba(144, 86, 86, 0.08) 50%, transparent 100%);
}

.speaker-row .info {
  margin-bottom: 0;
}

.speaker-photo {
  flex-shrink: 0;
  margin-right: 1rem;
}
</style>

<h1>Invited Speakers</h1>
<hr class="section-divider">

<p>We have a brilliant line-up of speakers.</p>

<h2>Control and Perception</h2>

<div class="speaker-row">
  <img src="/images/speakers_Lu.jpg" alt="Haojian Lu" class="speaker-photo">
  <div>
    <strong>Haojian Lu</strong><br>
    <div class="info">
      Professor at the Zhejiang University<br>
      Website: <a href="https://person.zju.edu.cn/en/luhaojian">ZJU Profile</a><br>      
      <strong style="font-size: 0.95rem; margin-top: 0.2rem; display: block;">NAVIAI: Humanoid robot for safe work and life</strong>
    </div>
    <p style="font-size: 0.7rem; color: rgba(65, 64, 64, 0.911); margin-top: 0.3rem; line-height: 1.3; text-align: justify;">
      Integrating the universal form of "humanoid robots" with "embodied intelligence" as their core, these systems achieve autonomous learning and behavioral generalization through environmental interaction, representing a key pathway to the scalable application of safe work and living. The team at Zhejiang Humanoid Robotics Innovation Center Inc. is dedicated to developing the high-precision, general humanoid robot "Navigator 2 NAVIAI," with a focus on breakthroughs in high-fidelity reconstruction, generalizable visual servo control, and task-centered long-horizon skill learning. Moving forward, the team will continue to prioritize the integration of spatial and embodied intelligence, abstract knowledge learning, and whole-body coordinated operation, advancing robotic technology toward higher levels of autonomy and safety in complex environments.
    </p>
  </div>
</div>

<div class="speaker-row">
  <img src="/images/speaker_andrea.jpg" alt="Andrea Pupa" class="speaker-photo">
  <div>
    <strong>Andrea Pupa</strong><br>
    <div class="info">
      Assistant Professor at the University of Modena<br>
      Website: <a href="https://www.arscontrol.unimore.it/andrea-pupa/">https://www.arscontrol.unimore.it/andrea-pupa/</a><br>
    </div>
    <p style="font-size: 0.7rem;  margin-top: 0.5rem; line-height: 1.2; text-align: justify;">     
    <strong style="font-size: 1rem;">Human-Robot Collaboration Enhanced: <br>Efficiency and Safety through Speed and Force Control</strong>
    </p>
    <p style="font-size: 0.7rem; color: rgba(65, 64, 64, 0.911); margin-top: 0.3rem; line-height: 1.2; text-align: justify;">     
      Human-robot collaboration has revolutionized modern manufacturing settings by combining human flexibility with robotic precision. To guarantee safety, however, the close contact between humans and robots is often translated into overly conservative robot motions. After a brief overview of how to assess safety in HRC from a regulatory point of view, this talk will present two different approaches to deal with safety. The first one, suitable for kinematic robots, reduces the robot's speed as it approaches the human operator, preventing collisions. The second one, suitable for torque-controlled robots, monitors and limits the system's energy to ensure that any contact or collision does not harm the human operator.
    </p>
  </div>
</div>

<h2>Modelling and Design</h2>

<div class="speaker-row">
  <img src="/images/speaker_IvanRuchkin.jpg" alt="Ivan Ruchkin" class="speaker-photo">
  <div>
    <strong>Ivan Ruchkin</strong><br>
    <div class="info">
      Assistant Professor, Department of Electrical and Computer Engineering, University of Florida<br>
      Website: <a href="https://ivan.ece.ufl.edu/">https://ivan.ece.ufl.edu/</a><br>
      Talk: <strong style="font-size: 1rem;">Reliable World Models: Physical Grounding and Safety Prediction</strong>
    </div>
    <p style="font-size: 0.7rem; color: rgba(65, 64, 64, 0.911); margin-top: 0.3rem; line-height: 1.3; text-align: justify;">
      Autonomous neural controllers pose critical safety challenges in modern robots. To support intervention and adaptation, robots should predict the safety of their future trajectories. However, it is difficult to do for long horizons, especially for rich sensor data under partial observability and distribution shift. The first part of this talk presents a family of deep-learning pipelines for calibrated safety prediction in end-to-end vision-controlled systems. Inspired by world models from reinforcement learning, our pipelines build upon variational autoencoders and recurrent predictors to forecast latent trajectories from raw image sequences. To overcome distribution shift due to compounding prediction errors and changing environmental conditions, we incorporate unsupervised domain adaptation.
    </p>
    <p style="font-size: 0.7rem; color: rgba(65, 64, 64, 0.911); margin-top: 0.5rem; line-height: 1.3; text-align: justify;">
      Unfortunately, learned latent representations in world models lack direct mapping to meaningful physical quantities, limiting their utility and interpretability in downstream planning, control, and safety verification. The second part of this talk argues for a fundamental shift from physically informed to physically interpretable world models. We crystallize four principles that leverage symbolic physical knowledge for interpretable world models: (1) structuring latent spaces, (2) aligning with invariances/equivariances, (3) exploiting supervision with varied strength and granularity, and (4) partitioning generative outputs. The final, third part of this talk dives into an interpretable world model for trajectory prediction. We discuss a novel architecture that aligns learned latent representations with real-world physical quantities by combining a physically interpretable image autoencoding model and a partially known dynamical model.
    </p>
  </div>
</div>

<div class="speaker-row">
  <img src="/images/speaker_sven.jpeg" alt="Sven Parusel" class="speaker-photo">
  <div>
    <strong>Sven Parusel</strong><br>
    <div class="info">
      Franka Robotics<br>
      Website: <a href="https://de.linkedin.com/in/sven-parusel-66210494">LinkedIn</a><br>
      <strong style="font-size: 0.95rem; margin-top: 0.2rem; display: block;">​Safe and Tactile Robots</strong>
    </div>
    <p style="font-size: 0.7rem; color: rgba(65, 64, 64, 0.911); margin-top: 0.3rem; line-height: 1.3; text-align: justify;">
      The era of cobots is coming to an end: the new ISO 10218 standards no longer defines collaborative robots, but rather collaborative applications. Robots are entering a new phase defined by tactility, intelligence, and safety by design. This session presents Franka Robotics' approach to safety – from mechanical design and force-sensitive control to modular, workflow-based safety configuration through the Watchman interface. Built upon over a decade of research in physical human-robot interaction, Franka's torque-controlled platforms exemplify a new generation of robots capable of understanding contact, ensuring safety while enabling contact-rich manipulation. The presentation also outlines current benchmarking initiatives on tactile performance and safety validation, highlighting progress towards a reference framework for safe and trustworthy AI Embodiments.
    </p>
  </div>
</div>

<div class="speaker-row">
  <img src="/images/speaker_MartaLa.jpeg" alt="Marta Lagomarsino" class="speaker-photo">
  <div>
    <strong>Marta Lagomarsino</strong><br>
    <div class="info">
      Istituto Italiano di Tecnologia (IIT)<br>
      Website: <a href="https://www.iit.it/people-details/-/people/marta-lagomarsino">IIT Profile</a><br>
      <strong style="font-size: 0.95rem; margin-top: 0.2rem; display: block;">Ergonomic and safe robots</strong>
    </div>
  </div>
</div>

<!-- h2>Acceptance and Certification</h2> -->
<h2>Deploying and Testing</h2>

<div class="speaker-row">
  <img src="/images/speaker_tapo.jpg" alt="Tapomayukh Bhattacharjee" class="speaker-photo">
  <div>
    <strong>Tapomayukh Bhattacharjee</strong><br>
    <div class="info">
      Assistant Professor at Cornell University<br>
      Website: <a href="https://emprise.cs.cornell.edu/">https://emprise.cs.cornell.edu/</a><br>
      <strong style="font-size: 0.95rem; margin-top: 0.2rem; display: block;">From Lab to Home: Safety in Deploying Physical Caregiving Robot Systems</strong>
    </div>
    <p style="font-size: 0.7rem; color: rgba(65, 64, 64, 0.911); margin-top: 0.3rem; line-height: 1.3; text-align: justify;">
      How can we build robots that safely and meaningfully assist people with mobility limitations in their daily lives? To support complex caregiving tasks such as robot-assisted feeding, transferring, and bathing, robots must physically interact with people and objects in dynamic, unstructured environments. As these systems transition from controlled lab settings into people's homes, ensuring their safety and reliability becomes paramount—especially when they are designed for intimate physical assistance. In this talk, I will present our lab's work toward safe and effective physical human–robot interaction in caregiving contexts. I will highlight approaches that enable robots to safely perceive, adapt, and respond during close-contact interactions with humans. Through these efforts, we examine both the technical and human-centered dimensions of safety—from modeling physical interactions and assessing user workload and comfort to understanding how people experience robotic assistance in real-world care settings.
    </p>
  </div>
</div>

<div class="speaker-row">
  <img src="/images/org_RobinKirschner.png" alt="Robin Kirschner" class="speaker-photo">
  <div>
    <strong>Robin Kirschner</strong><br>
    <div class="info">
      Technical University of Munich (TUM)<br>
      Website: <a href="https://www.ce.cit.tum.de/rsi/team/kirschner-robin/">TUM Profile</a><br>
      <strong style="font-size: 0.95rem; margin-top: 0.2rem; display: block;">Testing robot tactility</strong>
    </div>
  </div>
</div>
